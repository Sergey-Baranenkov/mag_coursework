{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/araxal/coursework\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T22:11:00.154656728Z",
     "start_time": "2023-05-08T22:11:00.154356437Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from models.genre_classification.sota_models.CNNSA import CNNSA\n",
    "import torchaudio\n",
    "from torchaudio.functional import resample\n",
    "import torch\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from utils.genre_classification import evaluate, executor\n",
    "from utils.decade_classification import feature_preparator\n",
    "from utils.genre_classification.plot_metrics import plot_metrics\n",
    "from utils.genre_classification import plot_confusion_matrix\n",
    "from torch import nn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T22:11:01.163463176Z",
     "start_time": "2023-05-08T22:11:00.154747491Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T22:11:01.183362733Z",
     "start_time": "2023-05-08T22:11:01.164384079Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 4\n",
    "DEVICE = torch.device('cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T22:11:01.197590601Z",
     "start_time": "2023-05-08T22:11:01.184164519Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.7 ms, sys: 49 µs, total: 6.75 ms\n",
      "Wall time: 6.69 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def raw_audio_slicer(features):\n",
    "    time_len = features.shape[1]\n",
    "    slice_len = 22050 * 5  # 5 минут\n",
    "\n",
    "    min_idx = np.random.randint(time_len - slice_len)\n",
    "\n",
    "    return torch.index_select(features, 1, torch.tensor(range(min_idx, min_idx + slice_len)))\n",
    "\n",
    "def transform(path: List[str]):\n",
    "    waveform, sample_rate = torchaudio.load(path[0], normalize=True)\n",
    "    waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    waveform = resample(waveform, orig_freq=sample_rate, new_freq=22050)\n",
    "    waveform = raw_audio_slicer(waveform)\n",
    "\n",
    "    return waveform[0]\n",
    "\n",
    "train_data_loader, val_data_loader, test_data_loader, idx_to_label = feature_preparator(\n",
    "    'features/decade_classification/external-nn.p',\n",
    "    BATCH_SIZE,\n",
    "    normalize=False,\n",
    "    external=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "def transform_idx_to_label(x):\n",
    "    return idx_to_label[x]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T22:11:01.241265464Z",
     "start_time": "2023-05-08T22:11:01.198584654Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([8, 110250])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_data_loader))[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T22:11:01.639287682Z",
     "start_time": "2023-05-08T22:11:01.241034254Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/araxal/.local/lib/python3.10/site-packages/torchaudio/functional/functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/100 [01:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [7], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m cnnsa_model \u001B[38;5;241m=\u001B[39m CNNSA(sample_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m22050\u001B[39m, n_class\u001B[38;5;241m=\u001B[39mNUM_CLASSES, f_max\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m11025\u001B[39m)\u001B[38;5;241m.\u001B[39mto(DEVICE)\n\u001B[0;32m----> 2\u001B[0m train_progress, val_progress \u001B[38;5;241m=\u001B[39m \u001B[43mexecutor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcnnsa_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtrain_data_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_data_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0001\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevaluate_per_iteration\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mearly_stop_after\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m15\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;241;43m0.001\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprint_metrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m plot_metrics(train_progress, val_progress, metrics \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m~/coursework/utils/genre_classification/executor.py:69\u001B[0m, in \u001B[0;36mexecutor\u001B[0;34m(device, model, train_dataloader, val_dataloader, epochs, learning_rate, evaluate_per_iteration, weight_decay, early_stop_after, lr_scheduler, print_metrics)\u001B[0m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m train_iter_num \u001B[38;5;241m%\u001B[39m evaluate_per_iteration \u001B[38;5;241m==\u001B[39m evaluate_per_iteration \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m     68\u001B[0m     val_iter_num \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 69\u001B[0m     val_loss, val_acc, _ \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_pred\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     70\u001B[0m     val_epoch_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m val_loss\n\u001B[1;32m     71\u001B[0m     val_epoch_accuracy \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m val_acc\n",
      "File \u001B[0;32m~/coursework/utils/genre_classification/evaluate.py:15\u001B[0m, in \u001B[0;36mevaluate\u001B[0;34m(device, model, dataloader, criterion, return_pred)\u001B[0m\n\u001B[1;32m     13\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 15\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m x_data, y_true \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[1;32m     16\u001B[0m         x_data, y_true \u001B[38;5;241m=\u001B[39m x_data\u001B[38;5;241m.\u001B[39mto(device), y_true\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     17\u001B[0m         pred_logits \u001B[38;5;241m=\u001B[39m model(x_data)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    633\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 634\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    636\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    638\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    676\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    677\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 678\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    679\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    680\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/coursework/utils/decade_classification/dataset.py:39\u001B[0m, in \u001B[0;36mDecadeClassificationDatasetForExternal.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     36\u001B[0m features, label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures_with_labels[idx]\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform:\n\u001B[0;32m---> 39\u001B[0m     features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray(features), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_to_idx[label]\n",
      "File \u001B[0;32m<timed exec>:10\u001B[0m, in \u001B[0;36mtransform\u001B[0;34m(path)\u001B[0m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torchaudio/backend/sox_io_backend.py:256\u001B[0m, in \u001B[0;36mload\u001B[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001B[0m\n\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ret \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    255\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ret\n\u001B[0;32m--> 256\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_fallback_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe_offset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_frames\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchannels_first\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torchaudio/io/_compat.py:108\u001B[0m, in \u001B[0;36mload_audio\u001B[0;34m(src, frame_offset, num_frames, convert, channels_first, format)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_audio\u001B[39m(\n\u001B[1;32m    100\u001B[0m     src: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m    101\u001B[0m     frame_offset: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    105\u001B[0m     \u001B[38;5;28mformat\u001B[39m: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    106\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, \u001B[38;5;28mint\u001B[39m]:\n\u001B[1;32m    107\u001B[0m     s \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mclasses\u001B[38;5;241m.\u001B[39mtorchaudio\u001B[38;5;241m.\u001B[39mffmpeg_StreamReader(src, \u001B[38;5;28mformat\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 108\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load_audio\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe_offset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_frames\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchannels_first\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torchaudio/io/_compat.py:88\u001B[0m, in \u001B[0;36m_load_audio\u001B[0;34m(s, frame_offset, num_frames, convert, channels_first)\u001B[0m\n\u001B[1;32m     86\u001B[0m option: Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m     87\u001B[0m s\u001B[38;5;241m.\u001B[39madd_audio_stream(i, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, _get_load_filter(frame_offset, num_frames, convert), \u001B[38;5;28;01mNone\u001B[39;00m, option)\n\u001B[0;32m---> 88\u001B[0m \u001B[43ms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess_all_packets\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     89\u001B[0m chunk \u001B[38;5;241m=\u001B[39m s\u001B[38;5;241m.\u001B[39mpop_chunks()[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunk \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "cnnsa_model = CNNSA(sample_rate=22050, n_class=NUM_CLASSES, f_max=11025).to(DEVICE)\n",
    "train_progress, val_progress = executor(DEVICE, cnnsa_model, train_dataloader = train_data_loader, val_dataloader=val_data_loader, epochs=100, learning_rate=0.0001, weight_decay=0.1, evaluate_per_iteration=15, early_stop_after=(15,  0.001), print_metrics=True)\n",
    "\n",
    "plot_metrics(train_progress, val_progress, metrics = ['loss', 'accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T22:12:15.172942282Z",
     "start_time": "2023-05-08T22:11:01.640263941Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loss, test_accuracy, (test_pred, test_true) = evaluate(DEVICE, cnnsa_model, test_data_loader, criterion=nn.CrossEntropyLoss(), return_pred=True)\n",
    "\n",
    "plot_confusion_matrix(test_true, test_pred, idx_to_label, transform_idx_to_label)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from models.genre_classification.sota_models.HarmonicCNN import HarmonicCNN\n",
    "\n",
    "harmonic_cnn_model = HarmonicCNN(sample_rate=22050, n_class=NUM_CLASSES, f_max=11025, n_channels=16).to(DEVICE)\n",
    "train_progress, val_progress = executor(DEVICE, harmonic_cnn_model, train_dataloader = train_data_loader, val_dataloader=val_data_loader, epochs=100, learning_rate=0.0001, weight_decay=0.1, evaluate_per_iteration=15, early_stop_after=(15,  0.001), print_metrics=True)\n",
    "\n",
    "plot_metrics(train_progress, val_progress, metrics = ['loss', 'accuracy'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loss, test_accuracy, (test_pred, test_true) = evaluate(DEVICE, harmonic_cnn_model, test_data_loader, criterion=nn.CrossEntropyLoss(), return_pred=True)\n",
    "\n",
    "plot_confusion_matrix(test_true, test_pred, idx_to_label, transform_idx_to_label)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
